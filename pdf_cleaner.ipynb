{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Document and get Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\gciprianherrera\\AppData\\Local\\Temp\\ipykernel_15268\\889672308.py:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  source = 'C:\\\\Users\\\\gciprianherrera\\\\Desktop\\\\LLM\\MVP_Chatbot\\\\PDF\\\\DEG\\\\ELH-2020-033633.pdf'\n"
     ]
    }
   ],
   "source": [
    "source = 'C:\\\\Users\\\\gciprianherrera\\\\Desktop\\\\LLM\\MVP_Chatbot\\\\PDF\\\\DEG\\\\ELH-2020-033633.pdf'\n",
    "# source = 'C:\\\\Users\\\\gciprianherrera\\\\Desktop\\\\LLM\\MVP_Chatbot\\\\PDF\\\\DEG\\\\ELH-2021-001129.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_file(source: str):\n",
    "    doc = fitz.open(source)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_title(lines, start_index, end_index):\n",
    "    title = \"Title not found\"\n",
    "    if lines[end_index].startswith((\"Page\", \"PAGE\", \"P1\"))==False:\n",
    "        title = lines[start_index+1:end_index-1]\n",
    "        title = \" \".join(title)\n",
    "    else:\n",
    "        title = lines[start_index+1:end_index]\n",
    "        title = \" \".join(title)\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lines_content(lines):\n",
    "    \"\"\"Merge lines where a line does not end with a period and the next starts with a lowercase letter.\"\"\"\n",
    "    merged_lines = []\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        if i < len(lines) - 1 and not lines[i].endswith('.') and lines[i+1][0].islower():\n",
    "            merged_lines.append(lines[i] + ' ' + lines[i+1])\n",
    "            i += 2\n",
    "        else:\n",
    "            merged_lines.append(lines[i])\n",
    "            i += 1\n",
    "    return merged_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_information(source):\n",
    "    \n",
    "    doc = get_pdf_file(source)\n",
    "    page = doc[0].get_text()\n",
    "    lines = page.split('\\n')\n",
    "    lines = [line for line in lines if line.strip()]\n",
    "    \n",
    "    \n",
    "    # Find the index of the line that contains the word \"TABLES DES MATIERES\"\n",
    "    toc_index = next((i for i, line in enumerate(lines) if \"TABLES DES MATIERES\" in line or \"TABLE DES MATIERES\" in line), None)\n",
    "    # Find the index where the references start\n",
    "    references_index = next((i for i, line in enumerate(lines) if re.match(r\"ELH-\\d{4}-\\d{6}(?:_\\d{1})?\", line)), None)\n",
    "    \n",
    "    # Find the index of the line that contains the word \"TABLES DES MATIERES\"\n",
    "    compte_rendu_index = next((i for i, line in enumerate(lines) if re.match(r\"\\bCOMPTE[-\\s]RENDU\\b\", line)), None)\n",
    "    \n",
    "    info_content = lines[:toc_index] + lines[references_index:]\n",
    "    # metadata['Title'] = find_title(info_content, compte_rendu_index, compte_rendu_index+3)\n",
    "    title = find_title(info_content, compte_rendu_index, compte_rendu_index+3)\n",
    "    \n",
    "    for line in info_content:\n",
    "        if re.match(r\"(\\d\\.\\d)\", line):\n",
    "            version = line\n",
    "            break\n",
    "        \n",
    "    for line in info_content:\n",
    "        if re.match(r\"\\b\\d{2}/\\d{2}/\\d{4}\\b\", line):\n",
    "            date = line\n",
    "            break\n",
    "    \n",
    "    for line in info_content:\n",
    "        if re.match(r\"(Diffusion (?:LimitÃ©e|Normale) Orano)\", line):\n",
    "            confidentiality = line\n",
    "            break\n",
    "        \n",
    "    perimeter = os.path.basename(os.path.dirname(source))\n",
    "    \n",
    "    investigation_number = os.path.basename(source).split(\".\")[0]\n",
    "    \n",
    "    metadata = {\n",
    "        \"Title\": title,\n",
    "        \"Version\": version,\n",
    "        \"Date\": date,\n",
    "        \"Confidentiality\": confidentiality,\n",
    "        \"Perimeter\": perimeter,\n",
    "        \"Investigation Number\": investigation_number\n",
    "    }\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_document_information(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_body(soure):\n",
    "    # Initialize indices\n",
    "    doc = get_pdf_file(source)\n",
    "    toc_index = None\n",
    "    references_index = None\n",
    "    page_content = []\n",
    "\n",
    "    for page in doc:\n",
    "        # Get text from the page\n",
    "        text = page.get_text()\n",
    "        \n",
    "        # Split text into lines and remove empty lines\n",
    "        lines = text.split('\\n')\n",
    "        lines = [line for line in lines if line.strip()]\n",
    "        lines = [line for line in lines if '\\uf0b7' not in line]\n",
    "        \n",
    "        # If the page is the first page\n",
    "        if page.number == 0:\n",
    "        \n",
    "            # Find the index of the line that contains the word \"TABLES DES MATIERES\"\n",
    "            toc_index = next((i for i, line in enumerate(lines) if \"TABLES DES MATIERES\" in line or \"TABLE DES MATIERES\" in line), None)\n",
    "\n",
    "            # toc_index = next((i for i, line in enumerate(lines) if re.match(r\"TABLE[S]? DE[S]? MATIERE[S]?\", line, re.IGNORECASE)), None)\n",
    "            # toc_index = next((i for i, line in enumerate(lines) if \"TABLES DES MATIERES\" in line), None)\n",
    "            \n",
    "            # Find the index where the references start\n",
    "            references_index = next((i for i, line in enumerate(lines) if re.match(r\"ELH-\\d{4}-\\d{6}(?:_\\d{1})?\", line)), None)\n",
    "            \n",
    "            # Assign default values if indices are None\n",
    "            toc_index = toc_index if toc_index is not None else -1\n",
    "            references_index = references_index if references_index is not None else len(lines)\n",
    "        \n",
    "        # For all pages, use the determined indices\n",
    "        references_index = next((i for i, line in enumerate(lines) if re.match(r\"ELH-\\d{4}-\\d{6}(?:_\\d{1})?\", line)), None)\n",
    "        content = lines[toc_index:references_index]\n",
    "        # page_content = add_hyperlink_to_figure_lines(content)\n",
    "        \n",
    "        # Merge lines content\n",
    "        while True:\n",
    "            new_content = merge_lines_content(content)\n",
    "            if new_content == content:\n",
    "                break\n",
    "            content = new_content \n",
    "        \n",
    "        page_content.extend(content)\n",
    "    \n",
    "    page_content = \"\\n\".join(page_content)\n",
    "    return page_content\n",
    "\n",
    "process_document_body(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_file(source: str):\n",
    "    doc = fitz.open(source)\n",
    "    return doc\n",
    "\n",
    "def add_hyperlink_to_figure_lines(lines):\n",
    "    \"\"\"Add hyperlinks to lines containing the word 'Figure' followed by a number.\"\"\"\n",
    "    figure_pattern = re.compile(r\"Figure \\d+\")\n",
    "    linked_lines = []\n",
    "    for line in lines:\n",
    "        if figure_pattern.search(line):\n",
    "            # Modify this line to include a hyperlink (example format)\n",
    "            line = f'<a href=\"#figure\"></a>{line}'\n",
    "        linked_lines.append(line)\n",
    "    return linked_lines\n",
    "\n",
    "def merge_lines_content(lines):\n",
    "    \"\"\"Merge lines where a line does not end with a period and the next starts with a lowercase letter.\"\"\"\n",
    "    merged_lines = []\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        if i < len(lines) - 1 and not lines[i].endswith('.') and lines[i+1][0].islower():\n",
    "            merged_lines.append(lines[i] + ' ' + lines[i+1])\n",
    "            i += 2\n",
    "        else:\n",
    "            merged_lines.append(lines[i])\n",
    "            i += 1\n",
    "    return merged_lines\n",
    "\n",
    "def find_title(lines, start_index, end_index):\n",
    "    title = \"Title not found\"\n",
    "    if lines[end_index].startswith((\"Page\", \"PAGE\", \"P1\"))==False:\n",
    "        title = lines[start_index+1:end_index-1]\n",
    "        title = \" \".join(title)\n",
    "    else:\n",
    "        title = lines[start_index+1:end_index]\n",
    "        title = \" \".join(title)\n",
    "    return title\n",
    "\n",
    "def process_document_body(soure):\n",
    "    # Initialize indices\n",
    "    doc = get_pdf_file(source)\n",
    "    toc_index = None\n",
    "    references_index = None\n",
    "    full_content = []\n",
    "\n",
    "    for page in doc:\n",
    "        # Get text from the page\n",
    "        text = page.get_text()\n",
    "        \n",
    "        # Split text into lines and remove empty lines\n",
    "        lines = text.split('\\n')\n",
    "        lines = [line for line in lines if line.strip()]\n",
    "        lines = [line for line in lines if '\\uf0b7' not in line]\n",
    "        \n",
    "        # If the page is the first page\n",
    "        if page.number == 0:\n",
    "            # Find the index of the line that contains the word \"TABLES DES MATIERES\"\n",
    "            toc_index = next((i for i, line in enumerate(lines) if re.match(r\"TABLE[S]? DE[S]? MATIERES?\", line)), None)\n",
    "            \n",
    "            # Find the index where the references start\n",
    "            references_index = next((i for i, line in enumerate(lines) if re.match(r\"ELH-\\d{4}-\\d{6}(?:_\\d{1})?\", line)), None)\n",
    "            \n",
    "            # Assign default values if indices are None\n",
    "            toc_index = toc_index if toc_index is not None else -1\n",
    "            references_index = references_index if references_index is not None else len(lines)\n",
    "        \n",
    "        # For all pages, use the determined indices\n",
    "        references_index = next((i for i, line in enumerate(lines) if re.match(r\"ELH-\\d{4}-\\d{6}(?:_\\d{1})?\", line)), None)\n",
    "        page_content = lines[toc_index:references_index]\n",
    "        page_content = add_hyperlink_to_figure_lines(page_content)\n",
    "        \n",
    "        # Merge lines content\n",
    "        while True:\n",
    "            new_page_content = merge_lines_content(page_content)\n",
    "            if new_page_content == page_content:\n",
    "                break\n",
    "            page_content = new_page_content \n",
    "        \n",
    "        full_content.extend(page_content)\n",
    "    \n",
    "    full_text = \"\\n\".join(full_content)\n",
    "    return full_text\n",
    "\n",
    "def process_document_information(source):\n",
    "    \n",
    "    doc = get_pdf_file(source)\n",
    "    page = doc[0].get_text()\n",
    "    lines = page.split('\\n')\n",
    "    lines = [line for line in lines if line.strip()]\n",
    "    \n",
    "    \n",
    "    # Find the index of the line that contains the word \"TABLES DES MATIERES\"\n",
    "    toc_index = next((i for i, line in enumerate(lines) if \"TABLES DES MATIERES\" in line), None)\n",
    "\n",
    "    # Find the index where the references start\n",
    "    references_index = next((i for i, line in enumerate(lines) if re.match(r\"ELH-\\d{4}-\\d{6}(?:_\\d{1})?\", line)), None)\n",
    "    \n",
    "    # Find the index of the line that contains the word \"TABLES DES MATIERES\"\n",
    "    compte_rendu_index = next((i for i, line in enumerate(lines) if re.match(r\"\\bCOMPTE[-\\s]RENDU\\b\", line)), None)\n",
    "    \n",
    "    info_content = lines[:toc_index] + lines[references_index:]\n",
    "    # metadata['Title'] = find_title(info_content, compte_rendu_index, compte_rendu_index+3)\n",
    "    title = find_title(info_content, compte_rendu_index, compte_rendu_index+3)\n",
    "    \n",
    "    for line in info_content:\n",
    "        if re.match(r\"(\\d\\.\\d)\", line):\n",
    "            version = line\n",
    "            break\n",
    "        \n",
    "    for line in info_content:\n",
    "        if re.match(r\"\\b\\d{2}/\\d{2}/\\d{4}\\b\", line):\n",
    "            date = line\n",
    "            break\n",
    "    \n",
    "    for line in info_content:\n",
    "        if re.match(r\"(Diffusion (?:LimitÃ©e|Normale) Orano)\", line):\n",
    "            confidentiality = line\n",
    "            break\n",
    "        \n",
    "    perimeter = os.path.basename(os.path.dirname(source))\n",
    "    \n",
    "    investigation_number = os.path.basename(source).split(\".\")[0]\n",
    "    \n",
    "    metadata = {\n",
    "        \"Title\": title,\n",
    "        \"Version\": version,\n",
    "        \"Date\": date,\n",
    "        \"Confidentiality\": confidentiality,\n",
    "        \"Perimeter\": perimeter,\n",
    "        \"Investigation Number\": investigation_number\n",
    "    }\n",
    "    return metadata\n",
    "\n",
    "# create a dictionnary with the metadata and the full text\n",
    "def process_document(source):\n",
    "    metadata = process_document_information(source)\n",
    "    full_text = process_document_body(source)\n",
    "    document = {\n",
    "        \"metadata\": metadata,\n",
    "        \"full_text\": full_text\n",
    "    }\n",
    "    return document\n",
    "\n",
    "process_document(source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import os\n",
    "\n",
    "class PDFDocumentProcessor:\n",
    "    def __init__(self, source: str):\n",
    "        source = source\n",
    "        doc = get_pdf_file()\n",
    "        toc_index = None\n",
    "        references_index = None\n",
    "        page_content = []\n",
    "    \n",
    "    def get_pdf_file(self):\n",
    "        doc = fitz.open(source)\n",
    "        return doc\n",
    "    \n",
    "    def add_hyperlink_to_figure_lines(self, lines):\n",
    "        \"\"\"Add hyperlinks to lines containing the word 'Figure' followed by a number.\"\"\"\n",
    "        figure_pattern = re.compile(r\"Figure \\d+\")\n",
    "        linked_lines = []\n",
    "        for line in lines:\n",
    "            if figure_pattern.search(line):\n",
    "                # Modify this line to include a hyperlink (example format)\n",
    "                line = f'<a href=\"#figure\"></a>{line}'\n",
    "            linked_lines.append(line)\n",
    "        return linked_lines\n",
    "\n",
    "    def merge_lines_content(self, lines):\n",
    "        \"\"\"Merge lines where a line does not end with a period and the next starts with a lowercase letter.\"\"\"\n",
    "        merged_lines = []\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            if i < len(lines) - 1 and not lines[i].endswith('.') and lines[i+1][0].islower():\n",
    "                merged_lines.append(lines[i] + ' ' + lines[i+1])\n",
    "                i += 2\n",
    "            else:\n",
    "                merged_lines.append(lines[i])\n",
    "                i += 1\n",
    "        return merged_lines\n",
    "    \n",
    "    def find_title(self, lines, start_index, end_index):\n",
    "        title = \"Title not found\"\n",
    "        if not lines[end_index].startswith((\"Page\", \"PAGE\", \"P1\")):\n",
    "            title = lines[start_index+1:end_index-1]\n",
    "            title = \" \".join(title)\n",
    "        else:\n",
    "            title = lines[start_index+1:end_index]\n",
    "            title = \" \".join(title)\n",
    "        return title\n",
    "    \n",
    "    def process_document_body(self):\n",
    "        for page in doc:\n",
    "            # Get text from the page\n",
    "            text = page.get_text()\n",
    "            \n",
    "            # Split text into lines and remove empty lines\n",
    "            lines = text.split('\\n')\n",
    "            lines = [line for line in lines if line.strip()]\n",
    "            lines = [line for line in lines if '\\uf0b7' not in line]\n",
    "            \n",
    "            # If the page is the first page\n",
    "            if page.number == 0:\n",
    "                # Find the index of the line that contains the word \"TABLES DES MATIERES\"\n",
    "                toc_index = next((i for i, line in enumerate(lines) if re.match(r\"TABLE[S]? DE[S]? MATIERES?\", line)), None)\n",
    "                \n",
    "                # Find the index where the references start\n",
    "                references_index = next((i for i, line in enumerate(lines) if re.match(r\"ELH-\\d{4}-\\d{6}(?:_\\d{1})?\", line)), len(lines))\n",
    "            \n",
    "            # For all pages, use the determined indices\n",
    "            references_index = next((i for i, line in enumerate(lines) if re.match(r\"ELH-\\d{4}-\\d{6}(?:_\\d{1})?\", line)), len(lines))\n",
    "            content = lines[toc_index:references_index]\n",
    "            content = add_hyperlink_to_figure_lines(content)\n",
    "            \n",
    "            # Merge lines content\n",
    "            while True:\n",
    "                new_content = merge_lines_content(content)\n",
    "                if new_content == content:\n",
    "                    break\n",
    "                content = new_content \n",
    "            \n",
    "            page_content.extend(content)\n",
    "        \n",
    "        page_content = \"\\n\".join(page_content)\n",
    "        return page_content\n",
    "\n",
    "    def process_document_information(self):\n",
    "        page = doc[0].get_text()\n",
    "        lines = page.split('\\n')\n",
    "        lines = [line for line in lines if line.strip()]\n",
    "        \n",
    "        # Find the index of the line that contains the word \"TABLES DES MATIERES\"\n",
    "        toc_index = next((i for i, line in enumerate(lines) if re.match(r\"TABLE[S]? DE[S]? MATIERES?\", line)), None)\n",
    "\n",
    "        # Find the index where the references start\n",
    "        references_index = next((i for i, line in enumerate(lines) if re.match(r\"ELH-\\d{4}-\\d{6}(?:_\\d{1})?\", line)), None)\n",
    "        \n",
    "        # Find the index of the line that contains the word \"TABLES DES MATIERES\"\n",
    "        compte_rendu_index = next((i for i, line in enumerate(lines) if re.match(r\"\\bCOMPTE[-\\s]RENDU\\b\", line)), None)\n",
    "        \n",
    "        info_content = lines[:toc_index] + lines[references_index:]\n",
    "        title = find_title(info_content, compte_rendu_index, compte_rendu_index+3)\n",
    "        \n",
    "        version = None\n",
    "        for line in info_content:\n",
    "            if re.match(r\"(\\d\\.\\d)\", line):\n",
    "                version = line\n",
    "                break\n",
    "        \n",
    "        date = None\n",
    "        for line in info_content:\n",
    "            if re.match(r\"\\b\\d{2}/\\d{2}/\\d{4}\\b\", line):\n",
    "                date = line\n",
    "                break\n",
    "        \n",
    "        confidentiality = None\n",
    "        for line in info_content:\n",
    "            if re.match(r\"(Diffusion (?:LimitÃ©e|Normale) Orano)\", line):\n",
    "                confidentiality = line\n",
    "                break\n",
    "        \n",
    "        perimeter = os.path.basename(os.path.dirname(source))\n",
    "        investigation_number = os.path.basename(source).split(\".\")[0]\n",
    "        \n",
    "        metadata = {\n",
    "            \"Title\": title,\n",
    "            \"Version\": version,\n",
    "            \"Date\": date,\n",
    "            \"Confidentiality\": confidentiality,\n",
    "            \"Perimeter\": perimeter,\n",
    "            \"Investigation Number\": investigation_number\n",
    "        }\n",
    "        return metadata\n",
    "\n",
    "    def process_document(self):\n",
    "        metadata = process_document_information()\n",
    "        full_text = process_document_body()\n",
    "        document = {\n",
    "            \"metadata\": metadata,\n",
    "            \"full_text\": full_text\n",
    "        }\n",
    "        return document\n",
    "\n",
    "# Usage\n",
    "source = \"C:\\\\Users\\\\gciprianherrera\\\\Desktop\\\\LLM\\\\MVP_Chatbot\\\\PDF\\\\DEG\\\\ELH-2019-017580.pdf\"\n",
    "processor = PDFDocumentProcessor(source)\n",
    "document = processor.process_document()\n",
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from Document import PDFDocumentProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_metadata_and_content(document):\n",
    "        metadata = document.get('metadata', {})\n",
    "        page_content = document.get('page_content', '')\n",
    "        return metadata, page_content\n",
    "\n",
    "data = []\n",
    "start_directory = \"C:\\\\Users\\\\gciprianherrera\\\\Desktop\\\\LLM\\\\MVP_Chatbot\\\\PDF\"\n",
    "\n",
    "for root, dirs, files in os.walk(start_directory):\n",
    "    for file in files:\n",
    "        if file.endswith('.pdf'):  # Adjust the file extension as needed\n",
    "            file_path = os.path.join(root, file)\n",
    "            processor = PDFDocumentProcessor(file_path)\n",
    "            document = processor.process_document()\n",
    "            metadata, page_content = extract_metadata_and_content(document)\n",
    "            row = {\n",
    "                'Title': metadata.get('Title'),\n",
    "                'Version': metadata.get('Version'),\n",
    "                'Date': metadata.get('Date'),\n",
    "                'Confidentiality': metadata.get('Confidentiality'),\n",
    "                'Perimeter': metadata.get('Perimeter'),\n",
    "                'Investigation Number': metadata.get('Investigation Number'),\n",
    "                'Page Content': page_content\n",
    "            }\n",
    "            data.append(row)\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataset import DatasetBuilder\n",
    "\n",
    "start_directory = \"C:\\\\Users\\\\gciprianherrera\\\\Desktop\\\\LLM\\\\MVP_Chatbot\\\\PDF\"\n",
    "pdf_processor = DatasetBuilder(start_directory)\n",
    "data = pdf_processor.process_directory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from semantic_router.encoders import OpenAIEncoder\n",
    "from semantic_chunkers import StatisticalChunker\n",
    "import pandas as pd\n",
    "\n",
    "# Function to initialize the encoder\n",
    "def initialize_encoder():\n",
    "    # Set the OpenAI API key from environment variable or prompt the user\n",
    "    os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "    # Initialize the encoder\n",
    "    encoder = OpenAIEncoder(name=\"text-embedding-3-large\")\n",
    "    \n",
    "    return encoder\n",
    "\n",
    "# Function to create a chunker\n",
    "def create_chunker(encoder):\n",
    "    return StatisticalChunker(encoder=encoder)\n",
    "\n",
    "# Function to process the dataset\n",
    "def process_dataset(data):\n",
    "    encoder = initialize_encoder()\n",
    "    chunker = create_chunker(encoder)\n",
    "    all_record_metadata = []\n",
    "\n",
    "    for idx, document in data.iterrows():\n",
    "        record_texts = chunker([document['Page Content']])\n",
    "        processed_texts = []\n",
    "        for chunk in record_texts[0]:\n",
    "            text_chunk = chunk.splits\n",
    "            text_chunk = \" \".join(text_chunk)\n",
    "            processed_texts.append(text_chunk)\n",
    "\n",
    "        metadata = {\n",
    "            'Title': document['Title'],\n",
    "            'Version': document['Version'],\n",
    "            'Date': document['Date'],\n",
    "            'Confidentiality': document['Confidentiality'],\n",
    "            'Perimeter': document['Perimeter'],\n",
    "            'Investigation Number': document['Investigation Number'],\n",
    "        }\n",
    "\n",
    "        record_metadata = [{\"chunk\": j, \"page_content\": text, **metadata} for j, text in enumerate(processed_texts)]\n",
    "        all_record_metadata.extend(record_metadata)\n",
    "\n",
    "    return all_record_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from semantic_router.encoders import OpenAIEncoder\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass(\n",
    "    \"OpenAI API key: \"\n",
    ")\n",
    "\n",
    "encoder = OpenAIEncoder(name=\"text-embedding-3-large\")\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_chunkers import StatisticalChunker\n",
    "\n",
    "chunker = StatisticalChunker(encoder=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(data):\n",
    "    all_record_metadata = []\n",
    "\n",
    "    for idx, document in data.iterrows():\n",
    "        record_texts = chunker([document['Page Content']])\n",
    "        processed_texts = []\n",
    "        for chunk in record_texts[0]:\n",
    "            text_chunk = chunk.splits\n",
    "            text_chunk = \" \".join(text_chunk)\n",
    "            processed_texts.append(text_chunk)\n",
    "\n",
    "        metadata = {\n",
    "            'Title': document['Title'],\n",
    "            'Version': document['Version'],\n",
    "            'Date': document['Date'],\n",
    "            'Confidentiality': document['Confidentiality'],\n",
    "            'Perimeter': document['Perimeter'],\n",
    "            'Investigation Number': document['Investigation Number'],\n",
    "        }\n",
    "\n",
    "        record_metadata = [{\"chunk\": j, \"page_content\": text, **metadata} for j, text in enumerate(processed_texts)]\n",
    "        all_record_metadata.extend(record_metadata)\n",
    "\n",
    "    return all_record_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from semantic_router.encoders import OpenAIEncoder\n",
    "from semantic_chunkers import StatisticalChunker\n",
    "import pandas as pd\n",
    "\n",
    "class DatasetChunker:\n",
    "    def __init__(self):\n",
    "        self.encoder = self.initialize_encoder()\n",
    "        self.chunker = self.create_chunker(self.encoder)\n",
    "    \n",
    "    def initialize_encoder(self):\n",
    "        # Set the OpenAI API key from environment variable\n",
    "        os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "        # Initialize the encoder\n",
    "        encoder = OpenAIEncoder(name=\"text-embedding-3-large\")\n",
    "        return encoder\n",
    "\n",
    "    def create_chunker(self, encoder):\n",
    "        return StatisticalChunker(encoder=encoder)\n",
    "\n",
    "    def process_dataset(self, data):\n",
    "        all_record_metadata = []\n",
    "\n",
    "        for idx, document in data.iterrows():\n",
    "            record_texts = self.chunker([document['Page Content']])\n",
    "            processed_texts = []\n",
    "            for chunk in record_texts[0]:\n",
    "                text_chunk = chunk.splits\n",
    "                text_chunk = \" \".join(text_chunk)\n",
    "                processed_texts.append(text_chunk)\n",
    "\n",
    "            metadata = {\n",
    "                'Title': document['Title'],\n",
    "                'Version': document['Version'],\n",
    "                'Date': document['Date'],\n",
    "                'Confidentiality': document['Confidentiality'],\n",
    "                'Perimeter': document['Perimeter'],\n",
    "                'Investigation Number': document['Investigation Number'],\n",
    "            }\n",
    "\n",
    "            record_metadata = [{\"chunk\": j, \"page_content\": text, **metadata} for j, text in enumerate(processed_texts)]\n",
    "            all_record_metadata.extend(record_metadata)\n",
    "\n",
    "        return all_record_metadata\n",
    "\n",
    "\n",
    "processor = DatasetChunker()\n",
    "result = processor.process_dataset(data[:5])\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-15 11:26:59 INFO semantic_chunkers.utils.logger Single document exceeds the maximum token limit of 300. Splitting to sentences before semantically merging.\u001b[0m\n",
      "100%|ââââââââââ| 4/4 [00:03<00:00,  1.12it/s]\n",
      "\u001b[32m2024-06-15 11:27:03 INFO semantic_chunkers.utils.logger Single document exceeds the maximum token limit of 300. Splitting to sentences before semantically merging.\u001b[0m\n",
      "100%|ââââââââââ| 4/4 [00:03<00:00,  1.29it/s]\n",
      "\u001b[32m2024-06-15 11:27:06 INFO semantic_chunkers.utils.logger Single document exceeds the maximum token limit of 300. Splitting to sentences before semantically merging.\u001b[0m\n",
      "100%|ââââââââââ| 4/4 [00:03<00:00,  1.14it/s]\n",
      "\u001b[32m2024-06-15 11:27:10 INFO semantic_chunkers.utils.logger Single document exceeds the maximum token limit of 300. Splitting to sentences before semantically merging.\u001b[0m\n",
      "100%|ââââââââââ| 4/4 [00:03<00:00,  1.11it/s]\n",
      "\u001b[32m2024-06-15 11:27:13 INFO semantic_chunkers.utils.logger Single document exceeds the maximum token limit of 300. Splitting to sentences before semantically merging.\u001b[0m\n",
      "100%|ââââââââââ| 5/5 [00:04<00:00,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "data_test = process_dataset(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initizalize OpenAI Azure models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "def get_azure_openai_response(prompt):\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "\n",
    "    # Initialize the AzureOpenAI client\n",
    "    completion_client = AzureOpenAI(\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        api_version=\"2023-05-15\",\n",
    "        azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "    )\n",
    "\n",
    "    # Get response from the model\n",
    "    response = completion_client.completions.create(\n",
    "        model=\"gpt-35-turbo\",\n",
    "        prompt=prompt,\n",
    "        temperature=0,\n",
    "        max_tokens=3000,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "\n",
    "    # Return the text of the first choice\n",
    "    return response.choices[0].text\n",
    "\n",
    "# Usage example\n",
    "# prompt = \"quelle est l'information sur le coronavirus ?\"\n",
    "# response_text = get_azure_openai_response(prompt)\n",
    "# print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass(\n",
    "    \"OpenAI API key: \"\n",
    ")\n",
    "\n",
    "client_emb = OpenAI()\n",
    "\n",
    "def get_embeddings(text):\n",
    "   embeddings = client_emb.embeddings.create(input = \"\"\"{}\"\"\".format(text), \n",
    "                                             model=\"text-embedding-ada-002\",\n",
    "                                             # length_function=tiktoken_len\n",
    "                                             )\n",
    "   return embeddings.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_embeddings(\"quelle est l'information sur le coronavirus ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initizalize Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_STORAGE_CONNECTION_STRING:str=os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "if not AZURE_STORAGE_CONNECTION_STRING.startswith(\"DefaultEndpointsProtocol=\"):\n",
    "    AZURE_STORAGE_CONNECTION_STRING = \"DefaultEndpointsProtocol=\" + AZURE_STORAGE_CONNECTION_STRING\n",
    "AZURE_STORAGE_CONTAINER_NAME=os.getenv(\"AZURE_STORAGE_CONTAINER_NAME\")\n",
    "AZURE_STORAGE_API_KEY=os.getenv(\"AZURE_STORAGE_API_KEY\")    \n",
    "AZURE_STORAGE_ENDPOINT=os.getenv(\"AZURE_STORAGE_ENDPOINT\")\n",
    "AZURE_STORAGE_DEPLOYEMENT_ID=os.getenv(\"AZURE_STORAGE_DEPLOYEMENT_ID\")\n",
    "blob_service_client = BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "def download_blob_to_file(blob_service_client: BlobServiceClient, container_name: str, blob_name: str):\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "    file_name = blob_name.split(\"/\")[-1]\n",
    "    \n",
    "    # Create a folder name with the -2 element of the blob_name if it does not exist in the downloaded_files folder\n",
    "    subfolder = blob_name.split(\"/\")[-2]\n",
    "    os.makedirs(os.path.join(r'blob_files', subfolder), exist_ok=True)\n",
    "    \n",
    "    file_path = os.path.join(r'blob_files', subfolder, file_name)\n",
    "    \n",
    "    # Check if file already exists\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File already exists: {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    with open(file_path, mode=\"wb\") as sample_blob:\n",
    "        download_stream = blob_client.download_blob()\n",
    "        sample_blob.write(download_stream.readall())\n",
    "    \n",
    "    return file_path\n",
    "\n",
    "def download_all_blobs_in_container(blob_service_client: BlobServiceClient, container_name: str):\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    blob_list = container_client.list_blobs()\n",
    "    \n",
    "    for blob in blob_list:\n",
    "        downloaded_file_path = download_blob_to_file(blob_service_client, container_name, blob.name)\n",
    "        if downloaded_file_path:\n",
    "            print(f\"Downloaded to: {downloaded_file_path}\")\n",
    "        else:\n",
    "            print(f\"Skipped: {blob.name}\")\n",
    "            \n",
    "download_all_blobs_in_container(blob_service_client, AZURE_STORAGE_CONTAINER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initialize Search AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    # ComplexField,\n",
    "    # CorsOptions,\n",
    "    SearchIndex,   \n",
    "    # ScoringProfile,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    VectorSearch,\n",
    "    VectorSearchProfile,\n",
    "    HnswAlgorithmConfiguration\n",
    "    )\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "import uuid\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_AI_SEARCH_API_KEY=os.getenv(\"AZURE_AI_SEARCH_API_KEY\")\n",
    "AZURE_AI_SEARCH_ENDPOINT=os.getenv(\"AZURE_AI_SEARCH_ENDPOINT\")\n",
    "AZURE_AI_SEARCH_DEPLOYEMENT_ID=os.getenv(\"AZURE_AI_SEARCH_DEPLOYEMENT_ID\")\n",
    "AZURE_AI_SEARCH_INDEX_NAME=os.getenv(\"AZURE_AI_SEARCH_INDEX_NAME\")\n",
    "AZURE_AI_SEARCH_INDEXER_NAME=os.getenv(\"AZURE_AI_SEARCH_INDEXER_NAME\")\n",
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_search = VectorSearch(\n",
    "        profiles=[VectorSearchProfile(name=\"my-vector-config\", algorithm_configuration_name=\"my-algorithms-config\")],\n",
    "        algorithms=[HnswAlgorithmConfiguration(name=\"my-algorithms-config\",\n",
    "                                               parameters={\"distanceMeasure\": \"Cosine\"})]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a search index\n",
    "\n",
    "index_client = SearchIndexClient(AZURE_AI_SEARCH_ENDPOINT, AzureKeyCredential(AZURE_AI_SEARCH_API_KEY))\n",
    "index_name = AZURE_AI_SEARCH_INDEX_NAME\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "    SearchableField(name=\"page_content\", type=SearchFieldDataType.String, analyzer_name=\"fr.lucene\"),\n",
    "    SearchField(name=\"embeddings\", \n",
    "                type=SearchFieldDataType.Collection(SearchFieldDataType.Single), \n",
    "                searchable = True,\n",
    "                vector_search_dimensions=1536,\n",
    "                vector_search_profile_name=\"my-vector-config\"),\n",
    "    SearchableField(name=\"Perimeter\", type=SearchFieldDataType.String, sortable=True, filterable=True),\n",
    "    SearchableField(name=\"Title\", type=SearchFieldDataType.String, analyzer_name=\"fr.lucene\"),\n",
    "    SearchableField(name=\"Version\", type=SearchFieldDataType.String, sortable=True, filterable=True),\n",
    "    # SearchableField(name=\"Investigation Number\", type=SearchFieldDataType.String, sortable=True, filterable=True),\n",
    "    SearchableField(name=\"Date\", type=SearchFieldDataType.String, sortable=True, filterable=True),\n",
    "]\n",
    "\n",
    "index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)\n",
    "index_client.create_index(index)\n",
    "# index_client.delete_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for doc in data_test:\n",
    "    docs.append({\"id\": str(uuid.uuid4()),\n",
    "                 \"page_content\": doc['page_content'],\n",
    "                 \"embeddings\": get_embeddings(doc['page_content']),\n",
    "                 \"Perimeter\": doc['Perimeter'],\n",
    "                 \"Title\": doc['Title'],\n",
    "                 \"Version\": doc['Version'],\n",
    "                 \"Date\": doc['Date'],\n",
    "                 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SearchClient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m search_client \u001b[38;5;241m=\u001b[39m \u001b[43mSearchClient\u001b[49m(AZURE_AI_SEARCH_ENDPOINT,index_name\u001b[38;5;241m=\u001b[39mAZURE_AI_SEARCH_INDEX_NAME, credential \u001b[38;5;241m=\u001b[39m AzureKeyCredential(AZURE_AI_SEARCH_API_KEY))\n\u001b[0;32m      2\u001b[0m result  \u001b[38;5;241m=\u001b[39m search_client\u001b[38;5;241m.\u001b[39mupload_documents(docs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SearchClient' is not defined"
     ]
    }
   ],
   "source": [
    "search_client = SearchClient(AZURE_AI_SEARCH_ENDPOINT,index_name=AZURE_AI_SEARCH_INDEX_NAME, credential = AzureKeyCredential(AZURE_AI_SEARCH_API_KEY))\n",
    "result  = search_client.upload_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"quelle zones sont les zones plus investigue dans le perimetre DEG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msearch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocuments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorizedQuery\n\u001b[1;32m----> 3\u001b[0m vector_query \u001b[38;5;241m=\u001b[39m VectorizedQuery(vector\u001b[38;5;241m=\u001b[39m\u001b[43mget_embeddings\u001b[49m(query),k_nearest_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,  fields\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m results \u001b[38;5;241m=\u001b[39m search_client\u001b[38;5;241m.\u001b[39msearch(\n\u001b[0;32m      6\u001b[0m     vector_queries\u001b[38;5;241m=\u001b[39m[vector_query],\n\u001b[0;32m      7\u001b[0m     select\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_content\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      8\u001b[0m     query_language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfr-fr\u001b[39m\u001b[38;5;124m\"\u001b[39m,)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.models import VectorizedQuery\n",
    "\n",
    "vector_query = VectorizedQuery(vector=get_embeddings(query),k_nearest_neighbors=20,  fields=\"embeddings\")\n",
    "\n",
    "results = search_client.search(\n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"page_content\"],\n",
    "    query_language=\"fr-fr\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpResponseError",
     "evalue": "() Invalid expression: Could not find a property named 'source' on type 'search.document'.\r\nParameter name: $select\nCode: \nMessage: Invalid expression: Could not find a property named 'source' on type 'search.document'.\r\nParameter name: $select",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m azure_client \u001b[38;5;241m=\u001b[39m AzureSearchQuery()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Perform a vectorized query\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[43mazure_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorized_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m input_text\n",
      "File \u001b[1;32mc:\\Users\\gciprianherrera\\Desktop\\LLM\\MVP_Chatbot\\MVP_final\\VectorQuery.py:66\u001b[0m, in \u001b[0;36mAzureSearchQuery.vectorized_query\u001b[1;34m(self, query, k_nearest_neighbors, fields, query_language)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m         input_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_content\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\gciprianherrera\\Desktop\\LLM\\MVP_Chatbot\\.venv\\Lib\\site-packages\\azure\\core\\paging.py:123\u001b[0m, in \u001b[0;36mItemPaged.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_page_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_page_iterator \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mby_page())\n\u001b[1;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_page_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gciprianherrera\\Desktop\\LLM\\MVP_Chatbot\\.venv\\Lib\\site-packages\\azure\\core\\paging.py:75\u001b[0m, in \u001b[0;36mPageIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnd of paging\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_next\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontinuation_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m AzureError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m error\u001b[38;5;241m.\u001b[39mcontinuation_token:\n",
      "File \u001b[1;32mc:\\Users\\gciprianherrera\\Desktop\\LLM\\MVP_Chatbot\\.venv\\Lib\\site-packages\\azure\\search\\documents\\_paging.py:124\u001b[0m, in \u001b[0;36mSearchPageIterator._get_next_cb\u001b[1;34m(self, continuation_token)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_next_cb\u001b[39m(\u001b[38;5;28mself\u001b[39m, continuation_token):\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continuation_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 124\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initial_query\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     _next_link, next_page_request \u001b[38;5;241m=\u001b[39m unpack_continuation_token(continuation_token)\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mdocuments\u001b[38;5;241m.\u001b[39msearch_post(search_request\u001b[38;5;241m=\u001b[39mnext_page_request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\gciprianherrera\\Desktop\\LLM\\MVP_Chatbot\\.venv\\Lib\\site-packages\\azure\\core\\tracing\\decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[1;32mc:\\Users\\gciprianherrera\\Desktop\\LLM\\MVP_Chatbot\\.venv\\Lib\\site-packages\\azure\\search\\documents\\_generated\\operations\\_documents_operations.py:786\u001b[0m, in \u001b[0;36mDocumentsOperations.search_post\u001b[1;34m(self, search_request, request_options, **kwargs)\u001b[0m\n\u001b[0;32m    784\u001b[0m     map_error(status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m=\u001b[39mresponse, error_map\u001b[38;5;241m=\u001b[39merror_map)\n\u001b[0;32m    785\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize\u001b[38;5;241m.\u001b[39mfailsafe_deserialize(_models\u001b[38;5;241m.\u001b[39mSearchError, pipeline_response)\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response\u001b[38;5;241m=\u001b[39mresponse, model\u001b[38;5;241m=\u001b[39merror)\n\u001b[0;32m    788\u001b[0m deserialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearchDocumentsResult\u001b[39m\u001b[38;5;124m\"\u001b[39m, pipeline_response)\n\u001b[0;32m    790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m:\n",
      "\u001b[1;31mHttpResponseError\u001b[0m: () Invalid expression: Could not find a property named 'source' on type 'search.document'.\r\nParameter name: $select\nCode: \nMessage: Invalid expression: Could not find a property named 'source' on type 'search.document'.\r\nParameter name: $select"
     ]
    }
   ],
   "source": [
    "from VectorQuery import AzureSearchQuery\n",
    "\n",
    "azure_client = AzureSearchQuery()\n",
    "\n",
    "# Perform a vectorized query\n",
    "input_text = azure_client.vectorized_query(query)\n",
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m: \n\u001b[1;32m----> 4\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\u001b[38;5;241m.\u001b[39mnext()\n\u001b[0;32m      5\u001b[0m         input_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_content\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "input_text = ''\n",
    "try:\n",
    "    while True: \n",
    "        result = results.next()\n",
    "        input_text += result[\"page_content\"] + \"\\n \"\n",
    "except StopIteration:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = f\"\"\"En utilisant le context suivant : {input_text}\\n \n",
    "il faut repondre a la question suivante : {query} seulment en francais\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_azure_openai_response(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
