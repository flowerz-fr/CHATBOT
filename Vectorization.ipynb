{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataset import DatasetBuilder\n",
    "\n",
    "start_directory = \"C:\\\\Users\\\\gciprianherrera\\\\Desktop\\\\LLM\\\\MVP_Chatbot\\\\PDF\"\n",
    "pdf_processor = DatasetBuilder(start_directory)\n",
    "data = pdf_processor.process_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_openai import AzureOpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_VERSION\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    model=os.getenv(\"AZURE_OPENAI_MODEL\"),\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings: AzureOpenAIEmbeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_EMB_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_EMB_API_KEY\"),\n",
    "    openai_api_version=os.getenv(\"AZURE_OPENAI_EMB_VERSION\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_EMB_DEPLOYMENT_NAME\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name: str = \"complete_dataset\"\n",
    "vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_key = os.getenv(\"AZURE_AI_SEARCH_API_KEY\"),\n",
    "    azure_search_endpoint = os.getenv(\"AZURE_AI_SEARCH_ENDPOINT\"),\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings.embed_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "loader = DataFrameLoader(data, page_content_column=\"page_content\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(text, disallowed_special=())\n",
    "    return len(tokens)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=5,\n",
    "    length_function=tiktoken_len,\n",
    "    separators=[\"\\n\", \" \", \"\"])\n",
    "splits = text_splitter.split_documents(documents)\n",
    "vector_store.add_documents(documents=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_AI_SEARCH_API_KEY=os.getenv(\"AZURE_AI_SEARCH_API_KEY\")\n",
    "AZURE_AI_SEARCH_ENDPOINT=os.getenv(\"AZURE_AI_SEARCH_ENDPOINT\")\n",
    "AZURE_AI_SEARCH_DEPLOYEMENT_ID=os.getenv(\"AZURE_AI_SEARCH_DEPLOYEMENT_ID\")\n",
    "AZURE_AI_SEARCH_INDEX_NAME=os.getenv(\"AZURE_AI_SEARCH_INDEX_NAME\")\n",
    "AZURE_AI_SEARCH_SERVICE_NAME=os.getenv(\"AZURE_AI_SEARCH_SERVICE_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "# from langchain import hub\n",
    "# from langchain_core.runnables import RunnableParallel\n",
    "from langchain_community.retrievers import (\n",
    "    AzureAISearchRetriever,   \n",
    ")\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2023-05-15\",\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    model=\"gpt-35-turbo\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "index_name: str = \"complete_dataset\"\n",
    "retriever = AzureAISearchRetriever(\n",
    "    content_key=\"content\", top_k=30, index_name=index_name\n",
    ")\n",
    "\n",
    "# query = \"Quel est l'objectif du compte-rendu de la cellule 951 DEG\"  \n",
    "# query = \"Quel est l'objectif de l'investigation sur l’ensemble porte-lame cellule 904\"  \n",
    "# query = \"Quels sont les équipements investigués sur le périmètre DEG\"  \n",
    "# query = \"Quel est le débit de dose de la 951 du DEG \" \n",
    "# query = \"Quelles sont les informations de la cellule 909 du DEG \" \n",
    "# query = \"Quelle est la quantité de boue de la fosse 217-02 du DEG\"  \n",
    "# query = \"DDD des PE de boues sur la fosse 217-02 du DEG \" \n",
    "query = \"Quels sont les enseignements sur les investigations de la cellule 959B du HADE\"\n",
    "# query = \"Quelles sont les informations sur l'état de la cellule 929A du périmètre HADE\"\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use five sentence maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "result = chain.invoke({\"input\": query})\n",
    "\n",
    "\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "def extract_list_sources(result):\n",
    "    sources = []\n",
    "    for doc in result['context']:\n",
    "        try:\n",
    "            data = doc.metadata['metadata']\n",
    "            data = json.loads(data)\n",
    "            source = data.get('Source')\n",
    "            if source:\n",
    "                sources.append(source)\n",
    "        except json.JSONDecodeError:\n",
    "            # Handle the case where metadata is not a valid JSON\n",
    "            continue\n",
    "    sources = list(OrderedDict.fromkeys(sources))\n",
    "    return sources\n",
    "\n",
    "\n",
    "# Creating JSON data\n",
    "json_result = {\n",
    "    \"input\": result['input'],\n",
    "    \"sources\": extract_list_sources(result),\n",
    "    \"answer\": result['answer']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "def extract_list_sources(result):\n",
    "    sources = []\n",
    "    for doc in result['context']:\n",
    "        try:\n",
    "            data = doc.metadata['metadata']\n",
    "            data = json.loads(data)\n",
    "            source = data.get('Source')\n",
    "            if source:\n",
    "                sources.append(source)\n",
    "        except json.JSONDecodeError:\n",
    "            # Handle the case where metadata is not a valid JSON\n",
    "            continue\n",
    "    sources = list(OrderedDict.fromkeys(sources))\n",
    "    return sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Quels sont les enseignements sur les investigations de la cellule 959B du HADE',\n",
       " 'sources': ['C:\\\\Users\\\\gciprianherrera\\\\Desktop\\\\LLM\\\\MVP_Chatbot\\\\PDF\\\\HADE\\\\ELH-2021-008655.pdf',\n",
       "  'C:\\\\Users\\\\gciprianherrera\\\\Desktop\\\\LLM\\\\MVP_Chatbot\\\\PDF\\\\HADE\\\\ELH-2019-036670.pdf',\n",
       "  'C:\\\\Users\\\\gciprianherrera\\\\Desktop\\\\LLM\\\\MVP_Chatbot\\\\PDF\\\\HADE\\\\ELH-2019-073706.pdf',\n",
       "  'C:\\\\Users\\\\gciprianherrera\\\\Desktop\\\\LLM\\\\MVP_Chatbot\\\\PDF\\\\STE2\\\\ELH-2023-026550.pdf',\n",
       "  'C:\\\\Users\\\\gciprianherrera\\\\Desktop\\\\LLM\\\\MVP_Chatbot\\\\PDF\\\\HADE\\\\ELH-2019-029034.pdf'],\n",
       " 'answer': \"Le compte-rendu présente les résultats des investigations réalisées dans le pot florentin 239-74 et dans la cuve 239-60 de la cellule 959B du bâtiment HADE. Les objectifs de ces investigations étaient de prélever un échantillon de dépôt présent au fond du pot 239-74 et de définir un profil radiologique du pot, ainsi qu'un pigeage du liquide résiduel présent dans la cuve 239-60 et une vidéo à 360°. Les opérations d'investigation ont été réalisées en février 2021 en utilisant les éléments suivants : N°DIMR : 428107, DT : 8910710, N° Spectre α : 39, N° Spectre βγ : 34. Les vidéos de l'intérieur du pot florentin et de la cuve 239-60 sont disponibles sous G/Invest. Une photographie extraite de la vidéo du pigeage est présentée dans le compte-rendu. Le compte-rendu présente également les opérations menées pour faciliter l'accessibilité de la cellule, telles que la dépose du mur de BBL pour la mise en place de la porte d'accès à la cellule 959B et le montage d'un échafaudage à l'intérieur de la cellule.\"}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import OrderedDict\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "from langchain_community.retrievers import AzureAISearchRetriever\n",
    "\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self):\n",
    "        self.llm = AzureChatOpenAI(\n",
    "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "            api_version=\"2023-05-15\",\n",
    "            azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "            model=\"gpt-35-turbo\",\n",
    "            temperature=0,\n",
    "        )\n",
    "\n",
    "        self.index_name = \"complete_dataset\"\n",
    "        self.retriever = AzureAISearchRetriever(\n",
    "            content_key=\"content\", top_k=30, index_name=self.index_name\n",
    "        )\n",
    "\n",
    "        system_prompt = (\n",
    "            \"Use the given context to answer the question. \"\n",
    "            \"If you don't know the answer, say you don't know. \"\n",
    "            \"Use five sentences maximum and keep the answer concise. \"\n",
    "            \"Context: {context}\"\n",
    "        )\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system_prompt),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "        self.question_answer_chain = create_stuff_documents_chain(self.llm, prompt)\n",
    "        self.chain = create_retrieval_chain(self.retriever, self.question_answer_chain)\n",
    "\n",
    "    def ask(self, user_query):\n",
    "        result = self.chain.invoke({\"input\": user_query})\n",
    "        \n",
    "        def extract_list_sources(result):\n",
    "                sources = []\n",
    "                for doc in result['context']:\n",
    "                    try:\n",
    "                        data = doc.metadata['metadata']\n",
    "                        data = json.loads(data)\n",
    "                        source = data.get('Source')\n",
    "                        if source:\n",
    "                            sources.append(source)\n",
    "                    except json.JSONDecodeError:\n",
    "                        # Handle the case where metadata is not a valid JSON\n",
    "                        continue\n",
    "                sources = list(OrderedDict.fromkeys(sources))  # Remove duplicates while preserving order\n",
    "                return sources\n",
    "        \n",
    "        # Create the json_result\n",
    "        json_result = {\n",
    "            \"input\": result['input'],\n",
    "            \"sources\": extract_list_sources(result),\n",
    "            \"answer\": result['answer']\n",
    "        }\n",
    "    \n",
    "        return json_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quelles sont les informations sur l'état de la cellule 929A du périmètre HADE\n",
      "La cellule 929A a été investiguée en avril et mai 2021 dans le cadre d'une opération de détalutage et de prélèvements de dépôts. Des vidéos de la cellule ont été réalisées et sont disponibles. La cellule mesure 3,15m de largeur, 3,62m de longueur et 2,80m de hauteur. Des mesures de dépression et de débit de dose ont été réalisées, ainsi que des prélèvements de dépôts avec un outil GODET. Les prélèvements ont un aspect sableux. Des mesures de débits de dose ont également été réalisées dans la cellule.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\gciprianherrera\\\\Desktop\\\\LLM\\\\MVP_Chatbot\\\\PDF\\\\HADE\\\\ELH-2021-028061.pdf'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from chat import Chatbot\n",
    "\n",
    "chatbot = Chatbot()\n",
    "# query = \"Quel est l'objectif du compte-rendu de la cellule 951 DEG\"  \n",
    "# query = \"Quel est l'objectif de l'investigation sur l’ensemble porte-lame cellule 904\"  \n",
    "# query = \"Quels sont les équipements investigués sur le périmètre DEG\"  \n",
    "# query = \"Quel est le débit de dose de la 951 du DEG \" \n",
    "# query = \"Quelles sont les informations de la cellule 909 du DEG \" \n",
    "# query = \"Quelle est la quantité de boue de la fosse 217-02 du DEG\"  \n",
    "# query = \"DDD des PE de boues sur la fosse 217-02 du DEG \" \n",
    "# query = \"Quels sont les enseignements sur les investigations de la cellule 959B du HADE\"\n",
    "query = \"Quelles sont les informations sur l'état de la cellule 929A du périmètre HADE\"\n",
    "response = chatbot.ask(query)\n",
    "print(response['input'])\n",
    "print(response['answer'])\n",
    "response['sources']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
